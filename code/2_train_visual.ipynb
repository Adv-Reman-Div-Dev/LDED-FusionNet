{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8aac515-40f0-4fc5-ad7f-49c7a1811409",
   "metadata": {},
   "source": [
    "# Multisensor fusion \n",
    "- Experimentï¼š Maraging Steel 300\n",
    "- Experiment number (single bead wall samples): 21-26\n",
    "- Recorded data: position, veolocity, coaxial ccd images, acoustic data\n",
    "- Defect generated: keyhole pores, cracks, defect-free\n",
    "\n",
    "### Notebook 2: training with only vision dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f5b265-a77f-4440-8cb9-0be328194d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from models import *\n",
    "from multimodaldataset import MultimodalDataset, LDEDAudioDataset, LDEDVisionDataset\n",
    "from utils import progress_bar\n",
    "\n",
    "\n",
    "Multimodal_dataset_PATH = os.path.join(\"C:\\\\Users\\\\Asus\\\\OneDrive_Chen1470\\\\OneDrive - Nanyang Technological University\\\\Dataset\\\\Multimodal_AM_monitoring\\\\LDED_Acoustic_Visual_Dataset\")\n",
    "CCD_Image_30Hz_path = os.path.join(Multimodal_dataset_PATH, 'Coaxial_CCD_images_30Hz')\n",
    "Audio_segmented_30Hz_PATH = os.path.join(Multimodal_dataset_PATH, 'Audio_signal_all_30Hz')\n",
    "Audio_raw_seg_PATH = os.path.join(Audio_segmented_30Hz_PATH, 'raw')\n",
    "Audio_equalized_seg_PATH = os.path.join(Audio_segmented_30Hz_PATH, 'equalized')\n",
    "Audio_bandpassed_seg_PATH = os.path.join(Audio_segmented_30Hz_PATH, 'bandpassed')\n",
    "Audio_denoised_seg_PATH = os.path.join(Audio_segmented_30Hz_PATH, 'denoised')\n",
    "AUDIO_DIR = Audio_denoised_seg_PATH\n",
    "VISON_DIR = CCD_Image_30Hz_path\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "ANNOTATIONS_FILE = os.path.join(Multimodal_dataset_PATH, \"vision_acoustic_label.csv\")\n",
    "\n",
    "classes = ('Defect-free', 'Cracks', 'Keyhole pores', 'Laser-off', 'Laser-start')\n",
    "SAMPLE_RATE = 44100\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c9c9a-336e-4c28-82d3-6df8c57f94b0",
   "metadata": {},
   "source": [
    "### Use GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a76d49-c958-4fc5-b061-c7bd029016aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1242f763-f544-46ac-b161-3da92dd3357b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>audio_file_name</th>\n",
       "      <th>image_file_name</th>\n",
       "      <th>class_ID</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sample22_0000.wav</td>\n",
       "      <td>sample22_frame000000.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sample22_0001.wav</td>\n",
       "      <td>sample22_frame000001.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sample22_0002.wav</td>\n",
       "      <td>sample22_frame000002.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sample22_0003.wav</td>\n",
       "      <td>sample22_frame000003.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sample22_0004.wav</td>\n",
       "      <td>sample22_frame000004.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index    audio_file_name           image_file_name  class_ID  \\\n",
       "0             0  Sample22_0000.wav  sample22_frame000000.jpg         3   \n",
       "1             1  Sample22_0001.wav  sample22_frame000001.jpg         3   \n",
       "2             2  Sample22_0002.wav  sample22_frame000002.jpg         3   \n",
       "3             3  Sample22_0003.wav  sample22_frame000003.jpg         3   \n",
       "4             4  Sample22_0004.wav  sample22_frame000004.jpg         3   \n",
       "\n",
       "  class_name  \n",
       "0  Laser-off  \n",
       "1  Laser-off  \n",
       "2  Laser-off  \n",
       "3  Laser-off  \n",
       "4  Laser-off  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df = pd.read_csv(ANNOTATIONS_FILE)\n",
    "annotations_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e81a204-f684-498b-a9b6-c5af79ea5ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1889 2413  844  389   80]\n"
     ]
    }
   ],
   "source": [
    "# Get the labels and count the number of samples for each class\n",
    "labels = annotations_df['class_ID'].values\n",
    "label_counts = np.unique(labels, return_counts=True)[1]\n",
    "print (label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f946658-876e-493f-9d57-03dc93521464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader, device):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "    \n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            _, y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n\n",
    "\n",
    "def plot_losses(train_losses, valid_losses):\n",
    "    '''\n",
    "    Function for plotting training and validation losses\n",
    "    '''\n",
    "    \n",
    "    # temporarily change the style of the plots to seaborn \n",
    "    plt.style.use('seaborn')\n",
    "\n",
    "    train_losses = np.array(train_losses) \n",
    "    valid_losses = np.array(valid_losses)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (8, 4.5))\n",
    "\n",
    "    ax.plot(train_losses, color='blue', label='Training loss') \n",
    "    ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "    ax.set(title=\"Loss over epochs\", \n",
    "            xlabel='Epoch',\n",
    "            ylabel='Loss') \n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    \n",
    "    # change the plot style to default\n",
    "    plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31462dee-a263-4a39-b3af-f31cd9858f95",
   "metadata": {},
   "source": [
    "## Define training, testing (evaluation) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef640ed5-0922-4375-b0cf-3e0d0892923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, epoch, trainloader, loss_fn, optimizer, device):\n",
    "    '''\n",
    "    Function for the training single epoch of the training loop\n",
    "    '''\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train() # training mode\n",
    "    running_loss = 0\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # for inputs, targets in trainloader:\n",
    "        # print(type(targets))\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # calculate loss (forward pass)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        # backpropagate error and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                    % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    epoch_loss = running_loss / len(trainloader.dataset)\n",
    "    print(\"---------------------------\")\n",
    "    return model, optimizer, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f9be35-1918-4a40-ab8d-3bfec8f34baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_epoch(model, epoch, testloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            ## forward pass and calculate loss\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # test_loss += loss.item()\n",
    "            test_loss += loss.item()\n",
    "            # _, predicted = outputs.max(1)\n",
    "            # total += targets.size(0)* inputs.size(0)\n",
    "            # correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #             % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    # acc = 100.*correct/total\n",
    "    # if acc > best_acc:\n",
    "    #     print('Saving..')\n",
    "    #     state = {\n",
    "    #         'model': model.state_dict(),\n",
    "    #         'acc': acc,\n",
    "    #         'epoch': epoch,\n",
    "    #     }\n",
    "    #     if not os.path.isdir('checkpoint'):\n",
    "    #         os.mkdir('checkpoint')\n",
    "    #     torch.save(state, './checkpoint/ckpt.pth')\n",
    "    #     best_acc = acc\n",
    "    \n",
    "    epoch_loss = test_loss / len(testloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69f8958-82a0-4e01-9dcc-d122e628b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, loss_fn, optimizer, train_loader, valid_loader, epochs, scheduler, device, print_every=1):\n",
    "    '''\n",
    "    Function defining the entire training loop\n",
    "    '''\n",
    "    \n",
    "    # set objects for storing metrics\n",
    "    best_loss = 1e10\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    " \n",
    "    # Train model\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        # training\n",
    "        model, optimizer, train_loss = train_single_epoch(model, epoch, train_loader, loss_fn, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        # with torch.no_grad():\n",
    "        model, valid_loss = test_single_epoch(model, epoch, valid_loader, loss_fn, device)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            \n",
    "            train_acc = get_accuracy(model, train_loader, device=device)\n",
    "            valid_acc = get_accuracy(model, valid_loader, device=device)\n",
    "                \n",
    "            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
    "                  f'Epoch: {epoch}\\t'\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    plot_losses(train_losses, valid_losses)\n",
    "    \n",
    "    return model, optimizer, (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eecc2d-2359-46c1-b436-4aa01ca573d3",
   "metadata": {},
   "source": [
    "## Preparing Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3fba84-b019-4257-91f7-d2b5c9e8d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------ transformation------\n",
    "train_transforms=transforms.Compose([\n",
    "    torchvision.transforms.Resize((32,32)), # original image size: (640,480)\n",
    "    transforms.RandomHorizontalFlip(),  # data augmentation\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[136.20371045013258], std=[61.9731240029325]),\n",
    "])\n",
    "\n",
    "val_transforms=transforms.Compose([\n",
    "    torchvision.transforms.Resize((32,32)), # original image size: (640,480)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[136.20371045013258], std=[61.9731240029325]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92dcd22-0efa-4dfc-b923-8b3dbbd9fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the total dataset:163\n"
     ]
    }
   ],
   "source": [
    "#------ data loader------\n",
    "# create the dataset for all samples\n",
    "visiondataset = LDEDVisionDataset(ANNOTATIONS_FILE,\n",
    "                                  VISON_DIR,\n",
    "                                  train_transforms,\n",
    "                                  device)\n",
    "print (\"length of the total dataset:\" + str(len(visiondataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ec9bc-9cce-4ab0-af11-c57ef0a1406b",
   "metadata": {},
   "source": [
    "### Dealing with Imbalanced dataset: stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3d6129-f387-4b94-b085-9851b0572cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# dealing with dataset imbalance: stratified sampling\n",
    "train_idx, val_idx, test_idx = [], [], []\n",
    "for class_idx, class_count in enumerate(label_counts):\n",
    "    indices = np.where(labels == class_idx)[0]\n",
    "    # Shuffle the indices for this class\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_split = int(train_ratio * class_count)\n",
    "    val_split  = int(val_ratio  * class_count) + train_split\n",
    "\n",
    "    train_idx.extend(indices[:train_split])\n",
    "    val_idx.extend(indices[train_split:val_split])\n",
    "    test_idx.extend(indices[val_split:])\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9639495c-332f-43c8-a761-5b19ff97655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for each dataset\n",
    "train_loader = DataLoader(LDEDVisionDataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(LDEDVisionDataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "test_loader = DataLoader(LDEDVisionDataset, batch_size=BATCH_SIZE, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625fa70-48b6-42b4-897e-932e1a93337d",
   "metadata": {},
   "source": [
    "### Dealing with Imbalanced dataset: stratified sampling (method 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03b9337-9d42-4612-8f96-16953fae6ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the train dataset:4492\n",
      "length of the val dataset:1123\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and validation datasets\n",
    "# train_annotations, val_annotations = train_test_split(annotations_df, test_size=0.2)\n",
    "# create the StratifiedShuffleSplit object\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "# split the indices of annotations into train and test sets\n",
    "train_indices, test_indices = next(sss.split(annotations_df, labels))\n",
    "\n",
    "# split the data into train and test sets\n",
    "train_annotations = annotations_df.iloc[train_indices, :]\n",
    "test_annotations = annotations_df.iloc[test_indices, :]\n",
    "\n",
    "# Create the train and validation datasets\n",
    "train_dataset = LDEDVisionDataset(train_annotations,\n",
    "                                  image_path = VISON_DIR,\n",
    "                                  image_transformation=train_transforms,\n",
    "                                  device=device)\n",
    "\n",
    "val_dataset = LDEDVisionDataset(test_annotations,\n",
    "                                image_path=VISON_DIR,\n",
    "                                image_transformation=val_transforms,\n",
    "                                device=device)\n",
    "\n",
    "# # Create train and val dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"length of the train dataset:\" +  str(len(train_dataset)))\n",
    "print(\"length of the val dataset:\" +  str(len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b27ee-d1f9-48da-8e22-d322f3d13f8e",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd3050c0-b69e-4aa0-8524-86e5350465bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "start_epoch = 0\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e6c6580-9182-4b69-8f2b-a7e0587f45ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: 442ms | Tot: 21s305ms | Loss: 4.893 | Acc: 35.018% (1573/449 141/141 \n",
      "loss: 4.567245960235596\n",
      "---------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# for epoch in range(start_epoch, start_epoch+EPOCHS):\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#     train_single_epoch(net, epoch, train_dataloader, loss_fn, optimizer, device)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#     test_single_epoch(net, epoch, val_dataloader, loss_fn, device)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#     scheduler.step()\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m model, optimizer, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, loss_fn, optimizer, train_loader, valid_loader, epochs, scheduler, device, print_every)\u001b[0m\n\u001b[0;32m     16\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model, valid_loss \u001b[38;5;241m=\u001b[39m test_single_epoch(model, epoch, valid_loader, loss_fn, device)\n\u001b[0;32m     21\u001b[0m valid_losses\u001b[38;5;241m.\u001b[39mappend(valid_loss)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m (print_every \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# -----Model---------------\n",
    "print('==> Building model..')\n",
    "net = VGG('VGG19')\n",
    "# net = LeNet() \n",
    "# net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "# net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "# net = EfficientNetB0()\n",
    "# net = RegNetX_200MF()\n",
    "# net = SimpleDLA()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Note: weight_decay in pytorch is L2 regularization\n",
    "# optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE,\n",
    "#                     momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "# for epoch in range(start_epoch, start_epoch+EPOCHS):\n",
    "#     train_single_epoch(net, epoch, train_dataloader, loss_fn, optimizer, device)\n",
    "#     test_single_epoch(net, epoch, val_dataloader, loss_fn, device)\n",
    "#     scheduler.step()\n",
    "\n",
    "model, optimizer, _ = training_loop(net, loss_fn, optimizer, train_dataloader, val_dataloader, EPOCHS, scheduler, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
