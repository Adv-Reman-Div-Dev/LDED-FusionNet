{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791f2627-ee73-4e0a-be02-f2aca1267016",
   "metadata": {},
   "source": [
    "# LDED Audiovisual Fusion \n",
    "\n",
    "Author: Chen Lequn.\n",
    "Created on 13 Sep 2023.\n",
    "\n",
    "- Material: Maraging Steel 300\n",
    "- Process: Robotic Llser-directed energy deposition\n",
    "- Recorded data: position, veolocity, coaxial ccd features, acoustic feature\n",
    "- Quality labels generated: keyhole pores, cracks, defect-free\n",
    "\n",
    "### Notebook 2: Feature extraction\n",
    "- Extract handcrafted features from video and audio stream\n",
    "- Vision features: melt pool geometric features, including width, length, moment of area, convex hull, etc.\n",
    "- Audio features: spectral centroid, spectral bandwidth, flux, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abb3f3-6292-413e-b442-07d572d373db",
   "metadata": {},
   "source": [
    "### System setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200b9ea2-6492-49ae-8c3e-8325602e4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "# Scikit learn\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle, resample, class_weight\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from collections import defaultdict\n",
    "\n",
    "## plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "488d1c5d-326d-4a72-88f5-60c4b4602cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[   INFO   ] MusicExtractorSVM: no classifier models were configured by default\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import essentia.standard as es\n",
    "from essentia.standard import Spectrum, Windowing, SpectralCentroidTime, SpectralComplexity, SpectralContrast\n",
    "from essentia.standard import Decrease, Energy, EnergyBandRatio, FlatnessDB, Flux, RollOff, StrongPeak, CentralMoments\n",
    "from essentia.standard import DistributionShape, Crest, MelBands, MFCC\n",
    "import soundfile as sf  # for reading audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66b2c9-2dde-4c1f-85dc-c968b33d4d42",
   "metadata": {},
   "source": [
    "https://essentia.upf.edu/algorithms_reference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3596831-f611-4595-8aa8-f413451e840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT_DIR = \"../\"\n",
    "IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, \"result_images\", 'feature_extraction')\n",
    "os.makedirs(IMAGE_PATH, exist_ok=True)\n",
    "\n",
    "Multimodal_dataset_PATH = \"/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset\"\n",
    "Dataset_path = os.path.join(Multimodal_dataset_PATH, f'25Hz')\n",
    "                            \n",
    "\n",
    "## function for automatically save the diagram/graph into the folder \n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 2.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd4c469f-a5fa-499a-8779-0e3fb3953442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_directories(base_path, sample_numbers):\n",
    "    sample_directories = []\n",
    "    for sample_number in sample_numbers:\n",
    "        sample_directories.append(os.path.join(base_path, f'{sample_number}'))\n",
    "    return sample_directories\n",
    "\n",
    "\n",
    "samples = [21, 22, 23, 24, 26, 32]\n",
    "sample_directories = get_sample_directories(Dataset_path, samples)\n",
    "\n",
    "# Get lists of image and audio directories for each sample\n",
    "image_directories = [os.path.join(sample_dir, 'images') for sample_dir in sample_directories]\n",
    "audio_directories = [os.path.join(sample_dir, 'equalized_audio') for sample_dir in sample_directories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a4748a6-5647-4622-b827-cdf939d3ebc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/21/images',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/22/images',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/23/images',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/24/images',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/26/images',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/32/images']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "966d55a7-3bcd-4461-9939-d6e776c17a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/21/equalized_audio',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/22/equalized_audio',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/23/equalized_audio',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/24/equalized_audio',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/26/equalized_audio',\n",
       " '/home/chenlequn/Dataset/LDED_acoustic_visual_monitoring_dataset/25Hz/32/equalized_audio']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "710bd9ef-e072-48ff-881f-f5ee59f4c00f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample index</th>\n",
       "      <th>audio_file_name</th>\n",
       "      <th>image_file_name</th>\n",
       "      <th>class_name</th>\n",
       "      <th>class_name_v2</th>\n",
       "      <th>Layer number</th>\n",
       "      <th>Sample number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sample_21_1.wav</td>\n",
       "      <td>sample_21_1.jpg</td>\n",
       "      <td>Laser-off</td>\n",
       "      <td>Laser-off</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sample_21_2.wav</td>\n",
       "      <td>sample_21_2.jpg</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>sample_21_3.wav</td>\n",
       "      <td>sample_21_3.jpg</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sample_21_4.wav</td>\n",
       "      <td>sample_21_4.jpg</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>sample_21_5.wav</td>\n",
       "      <td>sample_21_5.jpg</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13523</th>\n",
       "      <td>13524</td>\n",
       "      <td>sample_32_13524.wav</td>\n",
       "      <td>sample_32_13524.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13524</th>\n",
       "      <td>13525</td>\n",
       "      <td>sample_32_13525.wav</td>\n",
       "      <td>sample_32_13525.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13525</th>\n",
       "      <td>13526</td>\n",
       "      <td>sample_32_13526.wav</td>\n",
       "      <td>sample_32_13526.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526</th>\n",
       "      <td>13527</td>\n",
       "      <td>sample_32_13527.wav</td>\n",
       "      <td>sample_32_13527.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13527</th>\n",
       "      <td>13528</td>\n",
       "      <td>sample_32_13528.wav</td>\n",
       "      <td>sample_32_13528.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61994 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample index      audio_file_name      image_file_name   class_name  \\\n",
       "0                 1      sample_21_1.wav      sample_21_1.jpg    Laser-off   \n",
       "1                 2      sample_21_2.wav      sample_21_2.jpg  Defect-free   \n",
       "2                 3      sample_21_3.wav      sample_21_3.jpg  Defect-free   \n",
       "3                 4      sample_21_4.wav      sample_21_4.jpg  Defect-free   \n",
       "4                 5      sample_21_5.wav      sample_21_5.jpg  Defect-free   \n",
       "...             ...                  ...                  ...          ...   \n",
       "13523         13524  sample_32_13524.wav  sample_32_13524.jpg          NaN   \n",
       "13524         13525  sample_32_13525.wav  sample_32_13525.jpg          NaN   \n",
       "13525         13526  sample_32_13526.wav  sample_32_13526.jpg          NaN   \n",
       "13526         13527  sample_32_13527.wav  sample_32_13527.jpg          NaN   \n",
       "13527         13528  sample_32_13528.wav  sample_32_13528.jpg          NaN   \n",
       "\n",
       "      class_name_v2  Layer number  Sample number  \n",
       "0         Laser-off           1.0             21  \n",
       "1       Defect-free           1.0             21  \n",
       "2       Defect-free           1.0             21  \n",
       "3       Defect-free           1.0             21  \n",
       "4       Defect-free           1.0             21  \n",
       "...             ...           ...            ...  \n",
       "13523           NaN           NaN             32  \n",
       "13524           NaN           NaN             32  \n",
       "13525           NaN           NaN             32  \n",
       "13526           NaN           NaN             32  \n",
       "13527           NaN           NaN             32  \n",
       "\n",
       "[61994 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all annotation files into one DataFrame\n",
    "all_annotation_dfs = []\n",
    "for sample_dir, sample_number in zip(sample_directories, samples):\n",
    "    annotation_file = os.path.join(sample_dir, f'annotations_{sample_number}.csv')  # Update the file name\n",
    "    annotation_df = pd.read_csv(annotation_file)\n",
    "    all_annotation_dfs.append(annotation_df)\n",
    "combined_annotation_df = pd.concat(all_annotation_dfs)\n",
    "combined_annotation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917736c-bd39-46b2-8a24-c36ab02e38d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extracting melt pool visual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d361ee1-0bfd-4038-8c39-5f1c9871e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_contour_extraction(image, threshold=100):\n",
    "    \"\"\"\n",
    "    Extract general contour features from a given image.\n",
    "    \n",
    "    Parameters:\n",
    "        image (ndarray): The input image.\n",
    "        threshold (int): The threshold value for image processing.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted features.\n",
    "    \"\"\"\n",
    "    # Initialize the result dictionary with zeros\n",
    "    result = {\n",
    "        'max_contour_area': 0,\n",
    "        'rectangle_angle': 0,\n",
    "        'rectangle_width': 0,\n",
    "        'rectangle_height': 0,\n",
    "        'ellipse_angle': 0,\n",
    "        'ellipse_width': 0,\n",
    "        'ellipse_height': 0\n",
    "    }\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    src_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply blur\n",
    "    src_gray = cv2.blur(src_gray, (3, 3))\n",
    "    \n",
    "    # Apply threshold\n",
    "    _, threshold_output = cv2.threshold(src_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(threshold_output, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not contours:\n",
    "        return result  # Return result with zeros if no contours are found\n",
    "    \n",
    "    # Find the rotated rectangles and ellipses for each contour\n",
    "    min_rects = [cv2.minAreaRect(np.array(contour)) for contour in contours]\n",
    "    contour_areas = [cv2.contourArea(np.array(contour)) for contour in contours]\n",
    "    \n",
    "    # Get the index of the max contour area\n",
    "    max_contour_area_index = np.argmax(contour_areas)\n",
    "    max_contour_area = contour_areas[max_contour_area_index]\n",
    "    \n",
    "    # Store the max contour area\n",
    "    result['max_contour_area'] = max_contour_area\n",
    "    \n",
    "    # Store rectangle features\n",
    "    rect = min_rects[max_contour_area_index]\n",
    "    result['rectangle_angle'] = rect[-1]\n",
    "    result['rectangle_width'] = rect[1][0]\n",
    "    result['rectangle_height'] = rect[1][1]\n",
    "    \n",
    "    # Store ellipse features if enough points for fitEllipse\n",
    "    if len(contours[max_contour_area_index]) > 5:\n",
    "        ellipse = cv2.fitEllipse(np.array(contours[max_contour_area_index]))\n",
    "        result['ellipse_angle'] = ellipse[-1]\n",
    "        result['ellipse_width'] = ellipse[1][0]\n",
    "        result['ellipse_height'] = ellipse[1][1]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a9c382b-5b0d-4b85-b371-d930b1f7e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convex_hull_extract(frame, threshold=100):\n",
    "    \"\"\"\n",
    "    Extract convex hull features from a given image.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): The path to the image file.\n",
    "        threshold (int): The threshold value for binary conversion.\n",
    "    \n",
    "    Returns:\n",
    "        max_hull_area (float): The maximum area among all convex hulls.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to grayscale if the image is colored\n",
    "    if frame.shape[-1] > 1:\n",
    "        src_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        src_gray = frame\n",
    "\n",
    "    # Blur the image\n",
    "    src_gray = cv2.blur(src_gray, (3, 3))\n",
    "    \n",
    "    # Apply threshold\n",
    "    ret, threshold_output = cv2.threshold(src_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(threshold_output, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Initialize return values\n",
    "    max_hull_area = 0.0\n",
    "\n",
    "    # Check if any contour is detected\n",
    "    if contours:\n",
    "        # Find the convex hull object for each contour\n",
    "        hull = [cv2.convexHull(cnt) for cnt in contours]\n",
    "        \n",
    "        # Find the bounding convex hull area for each contour\n",
    "        hull_area = [cv2.contourArea(h) for h in hull]\n",
    "        \n",
    "        # Get the maximum convex hull area\n",
    "        max_hull_area = max(hull_area)\n",
    "        \n",
    "#         # Draw contours and convex hull on the original image (for visualization)\n",
    "#         drawing = np.zeros((threshold_output.shape[0], threshold_output.shape[1], 3), dtype=np.uint8)\n",
    "#         for i in range(len(contours)):\n",
    "#             color = (np.random.randint(0,256), np.random.randint(0,256), np.random.randint(0,256))\n",
    "#             cv2.drawContours(drawing, contours, i, color)\n",
    "#             cv2.drawContours(drawing, hull, i, color, 2)\n",
    "        \n",
    "#         # Show the output image with contours and convex hull\n",
    "#         plt.imshow(cv2.cvtColor(drawing, cv2.COLOR_BGR2RGB))\n",
    "#         plt.title('Contours and Convex Hull')\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "        \n",
    "    return max_hull_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30cc1e9a-16bf-407e-bdfe-e3bf27e2f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for moments\n",
    "def moment_extract(image, threshold):\n",
    "    # Initialize moments as zeros\n",
    "    features = {\n",
    "        'm00': 0,\n",
    "        'm10': 0,\n",
    "        'm01': 0,\n",
    "        'm20': 0,\n",
    "        'm11': 0,\n",
    "        'm02': 0,\n",
    "        'm30': 0,\n",
    "        'm21': 0,\n",
    "        'm12': 0,\n",
    "        'm03': 0,\n",
    "        'mu20': 0,\n",
    "        'mu11': 0,\n",
    "        'mu02': 0,\n",
    "        'mu30': 0,\n",
    "        'mu21': 0,\n",
    "        'mu12': 0,\n",
    "        'mu03': 0,\n",
    "        'nu20': 0,\n",
    "        'nu11': 0,\n",
    "        'nu02': 0,\n",
    "        'nu30': 0,\n",
    "        'nu21': 0,\n",
    "        'nu12': 0,\n",
    "        'nu03': 0,\n",
    "        'center_x': 0,\n",
    "        'center_y': 0,\n",
    "        'contour_area': 0,\n",
    "        'contour_length': 0\n",
    "    }\n",
    "    \n",
    "    # Convert to grayscale if the image is colored\n",
    "    if len(image.shape) > 2:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "\n",
    "    # Thresholding\n",
    "    _, thresh = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort contours by area\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        moments = cv2.moments(largest_contour)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if moments['m00'] != 0:\n",
    "            for moment_name, moment_value in moments.items():\n",
    "                features[moment_name] = moment_value\n",
    "                \n",
    "            features['center_x'] = moments['m10'] / moments['m00']\n",
    "            features['center_y'] = moments['m01'] / moments['m00']\n",
    "            features['contour_area'] = cv2.contourArea(largest_contour)\n",
    "            features['contour_length'] = cv2.arcLength(largest_contour, True)\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c0d09-884e-46c2-9cb1-9bb19369c87c",
   "metadata": {},
   "source": [
    "### Extract all visual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1c29e01-d803-4f8c-af1c-7b344ea5b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_visual_features(image_directories, threshold=100):\n",
    "    all_features_list = []\n",
    "    total_images = sum([len(os.listdir(img_dir)) for img_dir in image_directories if os.path.isdir(img_dir)])\n",
    "    pbar = tqdm(total=total_images, desc=\"Processing images\")\n",
    "\n",
    "    for img_dir in image_directories:\n",
    "        if os.path.isdir(img_dir):\n",
    "            for img_name in os.listdir(img_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(img_dir, img_name)\n",
    "                    img = cv2.imread(img_path)\n",
    "                    \n",
    "                    features_contour = general_contour_extraction(img, threshold=threshold)\n",
    "                    max_hull = convex_hull_extract(img, threshold=threshold)\n",
    "                    features_moments = moment_extract(img, threshold=threshold)\n",
    "                    \n",
    "                    # Merge all dictionaries into one\n",
    "                    merged_features = {'image_file_name': img_name, **features_contour, 'max_hull': max_hull, **features_moments}\n",
    "                    all_features_list.append(merged_features)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    return pd.DataFrame(all_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe19aed-e8f4-4a13-b16a-c29926a3ef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  13%|██▎               | 7800/61995 [00:38<04:30, 200.35it/s]"
     ]
    }
   ],
   "source": [
    "df_visual = extract_visual_features(image_directories)\n",
    "df_visual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b07648-fe82-4ca0-8299-aaaa3f686566",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a03079-13e4-4b87-8ede-33cc8d016ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = os.path.join(audio_directories[1], \"sample_22_9.wav\")\n",
    "audio_signal, sample_rate = sf.read(audio_path, dtype='float32')\n",
    "# print(sample_rate)\n",
    "# print (len(audio_signal))\n",
    "# print (len(audio_signal)/sample_rate)\n",
    "# plt.plot(audio_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4980f3e-bae6-4141-9a0d-114ce6cbb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio_lengths(audio_file_paths):\n",
    "    length_dict = defaultdict(list)\n",
    "    \n",
    "    for file_path in audio_file_paths:\n",
    "        audio_signal, sr = librosa.load(file_path, sr=None)\n",
    "        length_in_seconds = len(audio_signal) / sr\n",
    "        length_dict[length_in_seconds].append(file_path)\n",
    "        \n",
    "    if len(length_dict) == 1:\n",
    "        print(f\"All audio files have the same length: {list(length_dict.keys())[0]} seconds.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Not all audio files have the same length.\")\n",
    "        for length, files in length_dict.items():\n",
    "            print(f\"Length: {length} seconds -> Files: {files}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577546ae-7e4c-4e30-8a81-6465313601f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def example_usage_check_audio_lengths(audio_directories):\n",
    "    # Initialize an empty list to store audio file paths\n",
    "    audio_file_paths = []\n",
    "    \n",
    "    # Iterate over each directory in audio_directories to collect audio file paths\n",
    "    for directory in audio_directories:\n",
    "        for file_name in os.listdir(directory):\n",
    "            if file_name.endswith(\".wav\"):\n",
    "                audio_file_paths.append(os.path.join(directory, file_name))\n",
    "    \n",
    "    # Call the check_audio_lengths function\n",
    "    return check_audio_lengths(audio_file_paths)\n",
    "\n",
    "\n",
    "# Uncomment the line below to run the function\n",
    "example_usage_check_audio_lengths(audio_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9be446-594f-48cc-860d-d5238571ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_domain_features(audio_signal, sample_rate=44100):\n",
    "    \"\"\"\n",
    "    Extract time domain features from an audio signal using Essentia.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_signal: numpy array, the audio signal from which to extract features\n",
    "    - sample_rate: int, the sample rate of the audio signal\n",
    "    \n",
    "    Returns:\n",
    "    - features: dict, a dictionary containing the extracted features\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # RMS Energy\n",
    "    rms_algo = es.RMS()\n",
    "    rms_energy = rms_algo(audio_signal)\n",
    "    features['rms_energy'] = rms_energy\n",
    "    \n",
    "    # Amplitude Envelope\n",
    "    envelope_algo = es.Envelope()\n",
    "    amplitude_envelope = envelope_algo(audio_signal)\n",
    "    features['amplitude_envelope_mean'] = amplitude_envelope.mean()\n",
    "    features['amplitude_envelope_std'] = amplitude_envelope.std()\n",
    "    \n",
    "    # Zero Crossing Rate\n",
    "    zcr_algo = es.ZeroCrossingRate()\n",
    "    zero_crossing_rate = zcr_algo(audio_signal)\n",
    "    features['zero_crossing_rate'] = zero_crossing_rate\n",
    "    \n",
    "    # Dynamic Complexity and Loudness\n",
    "    dyn_algo = es.DynamicComplexity()\n",
    "    dynamic_complexity, loudness = dyn_algo(audio_signal)\n",
    "    features['dynamic_complexity'] = dynamic_complexity\n",
    "    features['loudness'] = loudness\n",
    "\n",
    "    # Loudness Vickers\n",
    "    loudness_algo = es.LoudnessVickers()\n",
    "    loudness_vickers = loudness_algo(audio_signal)\n",
    "    features['loudness_vickers'] = loudness_vickers\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3edd2-f87f-4b49-91bb-34b7444d8882",
   "metadata": {},
   "source": [
    "Essentia provides a variety of spectral descriptors that you can use for feature extraction:\n",
    "\n",
    "1. **Spectral Centroid**: Computes the center of mass of the spectrum.\n",
    "2. **Spectral Complexity**: Measures the amount of peak-like components in the spectrum.\n",
    "3. **Spectral Contrast**: Computes the spectral contrast features from an audio signal.\n",
    "4. **Spectral Decrease**: Computes the decrease of the spectrum.\n",
    "5. **Spectral Energy**: Computes the energy of the frequency domain signal.\n",
    "6. **Spectral Energy Band Ratio**: Computes the ratio of energy in specific bands to the total energy.\n",
    "7. **Spectral Flatness**: Computes the flatness of a spectrum.\n",
    "8. **Spectral Flux**: Computes the flux of the spectrum.\n",
    "9. **Spectral Rolloff**: Computes the rolloff frequency of an audio signal.\n",
    "10. **Spectral Strong Peak**: Computes the strong peak of the spectrum.\n",
    "12. **Spectral Variance, skewness, kurtosis**: Computes the variance of the spectral peaks.\n",
    "14. **MFCC (Mel Frequency Cepstral Coefficients)**: Widely used spectral feature in audio and speech processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf4b82-7ce5-49eb-b6cc-f2fe1feaeb84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_spectral_features(audio_signal, sample_rate, frame_size=1024, hop_size=512):\n",
    "    # Initialize the algorithms\n",
    "    window_algo = Windowing(type='hann')\n",
    "    spectrum_algo = Spectrum()\n",
    "    centroid_algo = SpectralCentroidTime(sampleRate=sample_rate)\n",
    "    complexity_algo = SpectralComplexity(sampleRate=sample_rate)\n",
    "    contrast_algo = SpectralContrast(frameSize=frame_size, highFrequencyBound=sample_rate/2, lowFrequencyBound=200, sampleRate=sample_rate)\n",
    "    decrease_algo = Decrease()\n",
    "    energy_algo = Energy()\n",
    "    energy_band_ratio_algo = EnergyBandRatio(sampleRate=sample_rate, stopFrequency=7000)\n",
    "    flatness_algo = FlatnessDB()\n",
    "    spectral_flux = Flux()\n",
    "    rolloff_algo = RollOff(sampleRate=sample_rate)\n",
    "    strong_peak_algo = StrongPeak()\n",
    "    central_moment_algo = CentralMoments()\n",
    "    distrubution_shape = DistributionShape()\n",
    "    spectral_crest_factor = Crest()\n",
    "    mel_bands_algo = MelBands()\n",
    "    mfcc_algo = MFCC(inputSize=hop_size+1, highFrequencyBound=sample_rate/2, numberCoefficients=13, sampleRate=sample_rate)\n",
    "    \n",
    "    # Initialize features dictionary with defaultdict to store lists\n",
    "    # features = {}\n",
    "    features = defaultdict(list)\n",
    "    \n",
    "    for frame in es.FrameGenerator(audio_signal, frameSize=frame_size, hopSize=hop_size):\n",
    "        windowed_frame = window_algo(frame)\n",
    "        spectrum = spectrum_algo(windowed_frame)\n",
    "\n",
    "        features['spectral_centroid'].append(centroid_algo(spectrum))\n",
    "        features['spectral_complexity'].append(complexity_algo(spectrum))\n",
    "        spectral_contrast, spectral_valley = contrast_algo(spectrum)\n",
    "        for i, val in enumerate(spectral_contrast):\n",
    "            features[f'spectral_contrast_{i}'].append(val)\n",
    "        for i, val in enumerate(spectral_valley):\n",
    "            features[f'spectral_valley_{i}'].append(val)\n",
    "        features['spectral_decrease'].append(decrease_algo(spectrum))\n",
    "        features['spectral_energy'].append(energy_algo(spectrum))\n",
    "        features['spectral_energy_band_ratio'].append(energy_band_ratio_algo(spectrum))\n",
    "        features['spectral_flatness'].append(flatness_algo(spectrum))\n",
    "        features['spectral_flux'].append(spectral_flux(spectrum))\n",
    "        features['spectral_rolloff'].append(rolloff_algo(spectrum))\n",
    "        features['spectral_strong_peak'].append(strong_peak_algo(spectrum))\n",
    "        central_moments = central_moment_algo(spectrum)\n",
    "        features['spectral_variance'].append(distrubution_shape(central_moments)[0])\n",
    "        features['spectral_skewness'].append(distrubution_shape(central_moments)[1])\n",
    "        features['spectral_kurtosis'].append(distrubution_shape(central_moments)[2])\n",
    "        features['spectral_crest_factor'].append(spectral_crest_factor(spectrum))\n",
    "\n",
    "        mfcc_bands, mfcc_coeffs = mfcc_algo(spectrum)\n",
    "        for i, coeff in enumerate(mfcc_coeffs):\n",
    "            features[f'mfcc_{i}'].append(coeff)\n",
    "            \n",
    "    # Prepare a dictionary to store mean and std separately\n",
    "    features_separated = {}\n",
    "    for key, value in features.items():\n",
    "        mean_val = np.mean(value)\n",
    "        std_val = np.std(value)\n",
    "        features_separated[f\"{key}_mean\"] = mean_val\n",
    "        features_separated[f\"{key}_std\"] = std_val\n",
    "    \n",
    "    return features_separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec417e-9cb9-406c-8b06-fcd73f98ad53",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sample_rate = 44100\n",
    "audio_signal = np.random.rand(4410).astype(np.float32)  \n",
    "features = extract_spectral_features(audio_signal, sample_rate, frame_size=1024)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c9c33-6369-4176-a55a-039516fe8121",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract all audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67061060-5287-442c-9500-7a3c8931ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_audio_features(audio_directories, frame_size=1024, hop_size=512):\n",
    "    all_features_list = []\n",
    "    \n",
    "    # Count total audio files for progress bar\n",
    "    total_audio_files = sum([len(os.listdir(audio_dir)) for audio_dir in audio_directories if os.path.isdir(audio_dir)])\n",
    "    \n",
    "    pbar = tqdm(total=total_audio_files, desc=\"Processing audio files\")\n",
    "\n",
    "    for audio_dir in audio_directories:\n",
    "        if os.path.isdir(audio_dir):\n",
    "            for audio_name in os.listdir(audio_dir):\n",
    "                if audio_name.lower().endswith(('.wav', '.flac', '.mp3')):\n",
    "                    audio_path = os.path.join(audio_dir, audio_name)\n",
    "                    \n",
    "                    # Read audio file\n",
    "                    audio_signal, sample_rate = sf.read(audio_path, dtype='float32')\n",
    "                    \n",
    "                    # Extract features\n",
    "                    time_domain_features = extract_time_domain_features(audio_signal, sample_rate)\n",
    "                    spectral_features = extract_spectral_features(audio_signal, sample_rate, frame_size, hop_size)\n",
    "                    \n",
    "                    # Merge all dictionaries into one\n",
    "                    merged_features = {'audio_file_name': audio_name, **time_domain_features, **spectral_features}\n",
    "                    all_features_list.append(merged_features)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    return pd.DataFrame(all_features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0696938-f4e7-4e6f-83c1-c4a151a134a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b263e8de-5085-4158-8595-c3b2de25ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain_features = extract_time_domain_features(audio_signal, sample_rate)\n",
    "time_domain_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9a897-d54f-4a3f-bafa-6037df84229a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_features_df = extract_all_audio_features(audio_directories, frame_size=1024, hop_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eee9bc-cd79-4465-9435-6ca1b1d16876",
   "metadata": {},
   "source": [
    "## Save extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a78129-3c61-4bd6-aa69-e1f65c9b01cb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51d882-fcba-46de-a899-38c6b0e851a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4ea9c-d478-4882-af2e-b25035410873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visual.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf8d5a-389b-4899-b443-06e55ade068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a429dccc-f652-48ba-8bc8-f46a3aa75ea7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample index</th>\n",
       "      <th>audio_file_name</th>\n",
       "      <th>image_file_name</th>\n",
       "      <th>class_name</th>\n",
       "      <th>class_name_v2</th>\n",
       "      <th>Layer number</th>\n",
       "      <th>Sample number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sample_21_1.wav</td>\n",
       "      <td>sample_21_1.jpg</td>\n",
       "      <td>Laser-off</td>\n",
       "      <td>Laser-off</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sample_21_2.wav</td>\n",
       "      <td>sample_21_2.jpg</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>sample_21_3.wav</td>\n",
       "      <td>sample_21_3.jpg</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sample_21_4.wav</td>\n",
       "      <td>sample_21_4.jpg</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>sample_21_5.wav</td>\n",
       "      <td>sample_21_5.jpg</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>Defect-free</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13523</th>\n",
       "      <td>13524</td>\n",
       "      <td>sample_32_13524.wav</td>\n",
       "      <td>sample_32_13524.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13524</th>\n",
       "      <td>13525</td>\n",
       "      <td>sample_32_13525.wav</td>\n",
       "      <td>sample_32_13525.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13525</th>\n",
       "      <td>13526</td>\n",
       "      <td>sample_32_13526.wav</td>\n",
       "      <td>sample_32_13526.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526</th>\n",
       "      <td>13527</td>\n",
       "      <td>sample_32_13527.wav</td>\n",
       "      <td>sample_32_13527.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13527</th>\n",
       "      <td>13528</td>\n",
       "      <td>sample_32_13528.wav</td>\n",
       "      <td>sample_32_13528.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61994 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample index      audio_file_name      image_file_name   class_name  \\\n",
       "0                 1      sample_21_1.wav      sample_21_1.jpg    Laser-off   \n",
       "1                 2      sample_21_2.wav      sample_21_2.jpg  Defect-free   \n",
       "2                 3      sample_21_3.wav      sample_21_3.jpg  Defect-free   \n",
       "3                 4      sample_21_4.wav      sample_21_4.jpg  Defect-free   \n",
       "4                 5      sample_21_5.wav      sample_21_5.jpg  Defect-free   \n",
       "...             ...                  ...                  ...          ...   \n",
       "13523         13524  sample_32_13524.wav  sample_32_13524.jpg          NaN   \n",
       "13524         13525  sample_32_13525.wav  sample_32_13525.jpg          NaN   \n",
       "13525         13526  sample_32_13526.wav  sample_32_13526.jpg          NaN   \n",
       "13526         13527  sample_32_13527.wav  sample_32_13527.jpg          NaN   \n",
       "13527         13528  sample_32_13528.wav  sample_32_13528.jpg          NaN   \n",
       "\n",
       "      class_name_v2  Layer number  Sample number  \n",
       "0         Laser-off           1.0             21  \n",
       "1       Defect-free           1.0             21  \n",
       "2       Defect-free           1.0             21  \n",
       "3       Defect-free           1.0             21  \n",
       "4       Defect-free           1.0             21  \n",
       "...             ...           ...            ...  \n",
       "13523           NaN           NaN             32  \n",
       "13524           NaN           NaN             32  \n",
       "13525           NaN           NaN             32  \n",
       "13526           NaN           NaN             32  \n",
       "13527           NaN           NaN             32  \n",
       "\n",
       "[61994 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0c1a0-15c8-4b0d-8924-dea2388f19ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the annotation dataframe with the audio and visual dataframes\n",
    "df_audiovisual = combined_annotation_df.merge(audio_features_df, how='left', on='audio_file_name')\n",
    "df_audiovisual = df_audiovisual.merge(df_visual, how='left', on='image_file_name')\n",
    "\n",
    "# Show the first few rows of the merged dataframe\n",
    "df_audiovisual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65576c6b-a27c-491a-9f60-e7e17d05b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['audio_file_name', 'image_file_name', 'class_name', 'class_name_v2']:\n",
    "    df_audiovisual[col] = df_audiovisual[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627c5cc-61a4-42fe-bbeb-2e94acd8a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audiovisual.to_hdf(os.path.join(Dataset_path, 'data_audiovisual_with_annotations(equalized_audio).h5'), key='df', mode='w', format='table')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
